{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "IPeCoNQgp4o_",
        "XyGNqHIvyuTu",
        "WlX_y-MNy6nl",
        "Dc-jPDky-tQA",
        "oEX8kMzb39Hb",
        "v0hoJfPgAC7v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "In this assignment, you will apply your knowledge of Recurrent Neural Networks (RNN) and\n",
        "PyTorch to build a sentiment analysis model. Sentiment analysis is a common Natural\n",
        "Language Processing (NLP) task where the objective is to classify sentences into different\n",
        "sentiment categories, such as positive, negative, or neutral. This task is widely used in various\n",
        "applications, including social media monitoring, customer feedback analysis, and market\n",
        "research.\n",
        "\n",
        "To achieve this, you will use the Stanford Sentiment Treebank (SST) dataset, a benchmark\n",
        "dataset in sentiment analysis. The assignment will guide you through downloading and\n",
        "preprocessing the SST dataset using the torchtext library, building a vocabulary, and\n",
        "splitting the dataset into training, validation, and test sets.\n",
        "\n",
        "You will then construct an RNN model to perform sentiment analysis. The model will be built\n",
        "using PyTorch, and you will be provided with several key hyperparameters, such as\n",
        "vocabulary size, embedding dimension, and hidden layer dimension. Your task is to complete\n",
        "the implementation of the RNN model, including the embedding layer, the recurrent layer,\n",
        "and the fully connected layer.\n",
        "After building the model, you will train it on the SST dataset and evaluate its performance on\n",
        "the validation and test sets. You are encouraged to experiment with different optimizers, such\n",
        "as SGD and Adam, and to fine-tune hyperparameters to improve the model's accuracy.\n",
        "\n",
        "By the end of this assignment, you will have a solid understanding of how to implement RNNs\n",
        "for sentiment analysis and how to optimize their performance using PyTorch. This hands-on\n",
        "experience will be valuable in applying deep learning techniques to various NLP tasks in your\n",
        "future projects."
      ],
      "metadata": {
        "id": "aqvy-Qnlnvvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Load and Preprocess the Data**\n",
        "In this step, we will load and preprocess the Stanford Sentiment Treebank (SST) dataset using\n",
        "the torchtext library.\n",
        "\n",
        "This involves several key tasks:\n",
        "\n",
        "**1. Import Required Libraries:** We start by importing necessary libraries, including\n",
        "torch for deep learning, torchtext for handling text data, and copy for handling data\n",
        "copying.\n",
        "\n",
        "**2. Define Fields:** We define two fields: TEXT and LABEL. The TEXT field handles the\n",
        "sentence input, specifying that the data is sequential, should be handled in batches,\n",
        "and should be converted to lowercase. The LABEL field handles the sentiment labels.\n",
        "\n",
        "**3. Load Data Splits:** Using the torchtext.datasets module, we load the SST dataset\n",
        "and split it into training, validation, and test sets. This is done using the\n",
        "datasets.SST.splits method, which takes the TEXT and LABEL fields as parameters.\n",
        "\n",
        "**4. Build Vocabulary:** We build the vocabulary for the TEXT and LABEL fields using the\n",
        "training data. This step involves creating a mapping of each unique word and label to\n",
        "a corresponding integer index. The build_vocab method of the Field class is used for\n",
        "this purpose.\n",
        "\n",
        "**5. Define Hyperparameters:** We define several hyperparameters that will be used in\n",
        "building the RNN model:\n",
        "\n",
        "– vocab_size: The size of the vocabulary, i.e., the number of unique words in the\n",
        "dataset.\n",
        "\n",
        "– label_size: The number of unique sentiment labels.\n",
        "\n",
        "– padding_idx: The index used for padding short sentences.\n",
        "\n",
        "– embedding_dim: The dimension of the word embeddings.\n",
        "\n",
        "– hidden_dim: The dimension of the hidden layer in the RNN.\n",
        "\n",
        "**6. Build Iterators:** We create iterators for the training, validation, and test sets using\n",
        "the data.BucketIterator.splits method. These iterators will yield batches of data\n",
        "during training and evaluation. The batch_size parameter specifies the number of\n",
        "samples in each batch.\n",
        "\n",
        "By the end of this step, we will have preprocessed the SST dataset, built the necessary\n",
        "vocabulary, and created data iterators to facilitate batch processing during model training\n",
        "and evaluation. This setup is essential for efficiently handling and processing the text data in\n",
        "subsequent steps.\n",
        "\n",
        "\n",
        "# **Step 2: Build an RNN Model for Sentiment Analysis**\n",
        "\n",
        "In this step, we will design and implement a Recurrent Neural Network (RNN) model to\n",
        "classify sentences into sentiment categories such as positive, negative, or neutral. RNNs are\n",
        "particularly well-suited for tasks involving sequential data, such as text, because they can\n",
        "capture temporal dependencies and contextual information within the sequences.\n",
        "To build our RNN model, we will use the following hyperparameters:\n",
        "• vocabulary size (vocab_size): The total number of unique words in our dataset.\n",
        "• embedding dimension (embedding_dim): The size of the dense vector\n",
        "representations for each word. This allows the model to capture semantic information\n",
        "about the words.\n",
        "• hidden layer dimension (hidden_dim): The number of units in the hidden layer of\n",
        "the RNN. This determines the capacity of the model to capture dependencies in the\n",
        "data.\n",
        "• number of layers (num_layers): The number of stacked recurrent layers in the\n",
        "model. Multiple layers can help capture more complex patterns in the data.\n",
        "• number of sentence labels (label_size): The number of unique sentiment labels in\n",
        "the dataset, which is the output size of the model.\n",
        "The key components of the RNN model will include:\n",
        "\n",
        "**1. Embedding Layer:** Converts input words into dense vectors of fixed size\n",
        "(embedding_dim). This layer helps in capturing semantic information about the words\n",
        "and reduces the dimensionality of the input data.\n",
        "\n",
        "**2. Recurrent Layer:** Processes the embedded word sequences to capture the temporal\n",
        "dependencies and contextual relationships within the sentences. This will be\n",
        "implemented using an RNN, LSTM, or GRU.\n",
        "\n",
        "**3. Fully Connected Layer:** Maps the output from the recurrent layer to the sentiment\n",
        "labels. This layer helps in making the final classification decision.\n",
        "\n",
        "You will need to implement the following parts of the model:\n",
        "\n",
        "• Initialization of Layers: Define and initialize the embedding, recurrent, and fully\n",
        "connected layers.\n",
        "\n",
        "• Forward Pass: Implement the forward function, which specifies how the input data passes through each layer of the model to produce the output.\n",
        "\n",
        "By completing this step, you will have a functional RNN model designed for sentiment analysis. This model will then be trained and evaluated on the SST dataset in subsequent steps. The performance of the model can be optimized by experimenting with different hyperparameters and training techniques.\n",
        "\n",
        "Below is the code provided for this step. You need to complete this code.\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "\n",
        "def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size,\n",
        "padding_idx):\n",
        "\n",
        "super(RNNClassifier, self).__init__()\n",
        "\n",
        "self.vocab_size = vocab_size\n",
        "\n",
        "self.embedding_dim = embedding_dim\n",
        "\n",
        "self.hidden_dim = hidden_dim\n",
        "\n",
        "self.label_size = label_size\n",
        "\n",
        "self.num_layers = 1\n",
        "\n",
        "\\# add the layers required for sentiment analysis.\n",
        "\n",
        "self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim,\n",
        "padding_idx=padding_idx)\n",
        "\n",
        "def zero_state(self, batch_size):\n",
        "\n",
        "\\# implement the function, which returns an initial hidden state.\n",
        "\n",
        "return None\n",
        "\n",
        "def forward(self, text):\n",
        "\n",
        "\\# implement the forward function of the model.\n",
        "\n",
        "embedding = self.embedding(text)\n",
        "\n",
        "return None\n",
        "\n",
        "# **Step 3: Train the RNN Model**\n",
        "\n",
        "In this step, we will train the RNN model using the Stanford Sentiment Treebank (SST) dataset. Training the model involves feeding the data into the network, computing the loss, performing backpropagation to calculate gradients, and updating the model's weights to minimize the loss.\n",
        "\n",
        "# **Step 4: Optimize Hyperparameters**\n",
        "In this step, we will focus on optimizing the hyperparameters of the RNN model to achieve better accuracy. Hyperparameters significantly impact the performance and efficiency of the model, so tuning them is essential. Here’s what needs to be done:\n",
        "\n",
        "**1. Experiment with Different Optimizers:**\n",
        "\n",
        "– Compare different optimization algorithms such as SGD (Stochastic Gradient\n",
        "Descent) and Adam (Adaptive Moment Estimation).\n",
        "\n",
        "– Assess the impact of each optimizer on the convergence speed and final\n",
        "accuracy of the model.\n",
        "\n",
        "**2. Adjust Learning Rate:**\n",
        "\n",
        "– Experiment with different learning rates for the chosen optimizer.\n",
        "\n",
        "– A too-high learning rate might cause the model to converge quickly to a\n",
        "suboptimal solution, while a too-low learning rate might make the training\n",
        "process unnecessarily slow.\n",
        "\n",
        "**3. Vary Batch Size:**\n",
        "\n",
        "– Try different batch sizes to see how they affect the training dynamics and\n",
        "\n",
        "model performance.\n",
        "– Larger batch sizes can lead to more stable gradient estimates but require more\n",
        "memory.\n",
        "\n",
        "**4. Modify Model Architecture:**\n",
        "\n",
        "– Experiment with different numbers of hidden units in the RNN layer to find the right balance between model capacity and computational efficiency.\n",
        "\n",
        "– Try stacking multiple RNN layers to capture more complex patterns in the data.\n",
        "\n",
        "**5. Incorporate Regularization Techniques:**\n",
        "\n",
        "– Use dropout layers to prevent overfitting by randomly setting a fraction of the input units to zero at each update during training.\n",
        "\n",
        "– Adjust the dropout rate to find the optimal value that reduces overfitting\n",
        "without significantly hindering training.\n",
        "\n",
        "**6. LR Scheduler:**\n",
        "\n",
        "– Implement a learning rate scheduler to gradually decrease the learning rate\n",
        "with increasing epochs, which can help achieve better convergence. More\n",
        "details can be found in the PyTorch documentation.\n",
        "\n",
        "**7. Saving the Best Model:**\n",
        "\n",
        "– Write code to save the model at the epoch with the highest validation accuracy to ensure that you retain the best-performing model.\n",
        "\n",
        "**8. Trying New Models:**\n",
        "\n",
        "– Explore different models, such as LSTM or GRU, to replace the RNN model. You\n",
        "can find details on recurrent layers in the PyTorch documentation."
      ],
      "metadata": {
        "id": "jA52XXCt1tRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Load and Preprocess the Data**"
      ],
      "metadata": {
        "id": "THi3UIW6-m4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Libraries**"
      ],
      "metadata": {
        "id": "VjOqdYoWpvxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall -y torchtext\n",
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpM1WxSfxPDk",
        "outputId": "cd9004f2-9aa5-4d52-ef4f-8b69f3b0c0ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n"
      ],
      "metadata": {
        "id": "T9TX0J_1omAX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Fields**\n",
        "Define the TEXT and LABEL fields for handling the input text and labels, respectively."
      ],
      "metadata": {
        "id": "IPeCoNQgp4o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.LabelField()"
      ],
      "metadata": {
        "id": "wt1DaKgzp9uo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Data Splits**\n",
        "Load the SST dataset and split it into training, validation, and test sets."
      ],
      "metadata": {
        "id": "XyGNqHIvyuTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data splits\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuZjfNw_qDYf",
        "outputId": "e6ba894b-9209-452a-f704-0dd88f5f32ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading trainDevTestTrees_PTB.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:01<00:00, 753kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build Vocabulary**\n",
        "Build the vocabulary for the TEXT and LABEL fields using the training data."
      ],
      "metadata": {
        "id": "WlX_y-MNy6nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary\n",
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)\n"
      ],
      "metadata": {
        "id": "Ykj7T4cly98f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define Hyperparameters and Build Iterators**\n",
        "Define hyperparameters and create data iterators for the training, validation, and test sets."
      ],
      "metadata": {
        "id": "a70UnCx3zGnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Build iterators\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        ")\n"
      ],
      "metadata": {
        "id": "snxs0a_SzBEc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2:** Build RNN"
      ],
      "metadata": {
        "id": "Dc-jPDky-tQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define the Model**\n",
        "Create the RNN model class with the embedding, recurrent, and fully connected layers."
      ],
      "metadata": {
        "id": "oEX8kMzb39Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = 1\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, self.label_size)\n",
        "\n",
        "    def zero_state(self, batch_size, device):\n",
        "        # Initialize the hidden state with zeros on the specified device\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        batch_size = text.size(0)\n",
        "        hidden = self.zero_state(batch_size, text.device)\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "        hidden = hidden[0].squeeze(0)\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "xuSLBNRj3LUx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation:**\n",
        "\n",
        "#### **Embedding Layer:**\n",
        "\n",
        "**Function:** The embedding layer converts input words into dense vectors of a fixed size (embedding_dim).\n",
        "\n",
        "**Purpose:** This layer helps in capturing semantic information about the words and reduces the dimensionality of the input data, making it suitable for processing by the LSTM layer.\n",
        "\n",
        "**Details:** It takes as input the integer-encoded words from the vocabulary and transforms them into dense vectors using learned embeddings. The padding_idx parameter ensures that the padding token does not affect the model training.\n",
        "\n",
        "#### **LSTM Layer:**\n",
        "\n",
        "**Function:** The LSTM (Long Short-Term Memory) layer processes the embedded word sequences to capture temporal dependencies and contextual relationships within the sentences.\n",
        "\n",
        "**Purpose:** LSTMs are capable of learning long-term dependencies in sequences, which is crucial for understanding the context in sentences for sentiment analysis.\n",
        "\n",
        "**Details:** It takes the embedded sequences as input and produces hidden states for each time step. The hidden state from the final time step is used for sentiment classification. The batch_first=True argument indicates that the input tensor will have the batch size as the first dimension.\n",
        "\n",
        "#### **Fully Connected Layer:**\n",
        "\n",
        "**Function:** The fully connected layer maps the output from the LSTM layer to the sentiment labels.\n",
        "\n",
        "**Purpose:** This layer translates the learned features from the LSTM into a prediction for the sentiment class.\n",
        "\n",
        "**Details:** It takes the final hidden state from the LSTM and passes it through a linear layer to produce the logits for each sentiment class.\n",
        "\n",
        "#### **zero_state Function:**\n",
        "\n",
        "**Function:** The zero_state function initializes the hidden state of the LSTM with zeros.\n",
        "\n",
        "**Purpose:** This function ensures that the hidden state is reset at the beginning of each new sequence in a batch.\n",
        "\n",
        "**Details:** It returns a tuple containing two tensors of zeros, representing the initial hidden state and cell state of the LSTM.\n",
        "\n",
        "#### **Forward Function:**\n",
        "\n",
        "**Function:** The forward function defines the forward pass of the model, specifying how the input data passes through each layer of the model to produce the output.\n",
        "\n",
        "**Purpose:** This function outlines the data flow through the embedding layer, LSTM layer, and fully connected layer to generate the final output.\n",
        "\n",
        "**Details:** The input text is first converted to embeddings, then passed through the LSTM layer. The final hidden state from the LSTM is fed into the fully connected layer to produce the sentiment predictions."
      ],
      "metadata": {
        "id": "UV3vnySN4rt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3:** Train the RNN Model"
      ],
      "metadata": {
        "id": "kCS-2ZYv_NWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialize the Model, Optimizer, and Loss Function**"
      ],
      "metadata": {
        "id": "v0hoJfPgAC7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "lvugX1bUAJAi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define Training and Evaluation Functions**\n"
      ],
      "metadata": {
        "id": "VDpWvZHnASTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, labels = batch.text.to(device), batch.label.to(device)\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, labels = batch.text.to(device), batch.label.to(device)\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.argmax(preds, dim=1)\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "4Q20XELmAZZB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training Loop**"
      ],
      "metadata": {
        "id": "32WWuhsOAejf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model, val_iter, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {val_loss:.3f}, Val. Acc: {val_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZptFQSWAc7t",
        "outputId": "47fbb57f-7d4d-41ea-9e18-abc2b9ee7c44"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 1.035, Train Acc: 45.61%, Val. Loss: 0.996, Val. Acc: 53.64%\n",
            "Epoch: 02, Train Loss: 0.912, Train Acc: 58.51%, Val. Loss: 0.936, Val. Acc: 58.10%\n",
            "Epoch: 03, Train Loss: 0.727, Train Acc: 69.69%, Val. Loss: 0.922, Val. Acc: 61.27%\n",
            "Epoch: 04, Train Loss: 0.538, Train Acc: 78.52%, Val. Loss: 1.038, Val. Acc: 58.45%\n",
            "Epoch: 05, Train Loss: 0.352, Train Acc: 86.79%, Val. Loss: 1.268, Val. Acc: 56.67%\n",
            "Epoch: 06, Train Loss: 0.190, Train Acc: 93.68%, Val. Loss: 1.529, Val. Acc: 59.92%\n",
            "Epoch: 07, Train Loss: 0.103, Train Acc: 96.85%, Val. Loss: 1.902, Val. Acc: 57.74%\n",
            "Epoch: 08, Train Loss: 0.064, Train Acc: 98.21%, Val. Loss: 2.159, Val. Acc: 55.78%\n",
            "Epoch: 09, Train Loss: 0.045, Train Acc: 98.54%, Val. Loss: 2.202, Val. Acc: 56.90%\n",
            "Epoch: 10, Train Loss: 0.028, Train Acc: 99.24%, Val. Loss: 2.335, Val. Acc: 57.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Evaluate the Model on Test Data**"
      ],
      "metadata": {
        "id": "2U8gH1HrAnoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_iter, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16Dl82w6AnSA",
        "outputId": "01252cb0-fd6c-4aed-c9ca-579151f5d926"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.325, Test Acc: 58.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results indicate that the model is overfitting. This is evident from the decreasing training loss and increasing training accuracy, while the validation loss increases and validation accuracy fluctuates or decreases as training progresses. Overfitting occurs when the model learns the training data too well, including noise and details that do not generalize well to unseen data.\n",
        "\n",
        "Strategies to Address Overfitting\n",
        "\n",
        "1. Tune Hyperparameters:\n",
        "Experiment with learning rate, batch size, optimizer, epochs, and hidden dimensions.\n",
        "2. Try New Models:\n",
        "Explore other recurrent layers such as GRU and self-attention mechanisms.\n",
        "\n",
        "3. Save the Best Model:\n",
        "Write code to save the model at the epoch with the highest validation accuracy.\n",
        "\n",
        "4. Increase Dropout Rate:\n",
        "Increasing the dropout rate can help by reducing the model's dependency on specific neurons during training.\n",
        "\n",
        "5. Regularization:\n",
        "Incorporate L2 regularization in the optimizer to penalize large weights.\n"
      ],
      "metadata": {
        "id": "1qFOaL1d6ywe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Optimization**"
      ],
      "metadata": {
        "id": "aPgA8k0-ERaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will:\n",
        "\n",
        "Tune Hyperparameters: Experiment with learning rate, batch size, optimizer, epochs, and hidden dimensions.\n",
        "\n",
        "Try New Models: Explore other recurrent layers such as GRU and self-attention mechanisms.\n",
        "\n",
        "Use Learning Rate Scheduler: Gradually decrease the learning rate as epochs increase.\n",
        "\n",
        "Save the Best Model: Write code to save the model at the epoch with the highest validation accuracy."
      ],
      "metadata": {
        "id": "H8XB3xDQEZnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modify the Model to Include GRU Layer and Self-Attention**\n",
        "First, we will modify the model to include a GRU layer and an optional self-attention mechanism"
      ],
      "metadata": {
        "id": "NJp7ERg5ElKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, dropout=0.5):\n",
        "        super(GRUClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = 1\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # GRU layer with dropout\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, self.label_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        batch_size = text.size(0)\n",
        "        hidden = self.init_hidden(batch_size, text.device)\n",
        "        gru_out, hidden = self.gru(embedded, hidden)\n",
        "        hidden = self.dropout(hidden[-1])\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model with increased dropout\n",
        "model = GRUClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, dropout=0.5).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYTtkFQbEjr1",
        "outputId": "fa88835a-5b2c-4b37-f3b9-5ac05bc305a6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define Training and Evaluation Functions**"
      ],
      "metadata": {
        "id": "pEKOvEWBEysc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, labels = batch.text.to(device), batch.label.to(device)\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, labels = batch.text.to(device), batch.label.to(device)\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.argmax(preds, dim=1)\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "co9RD2Wn9M2Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Loop with Dropout, Learning Rate Scheduler, and Model Saving**\n",
        "We will write code to save the model at the epoch with the highest validation accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "C2GIBTfOE7DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "num_epochs = 10\n",
        "best_val_acc = 0.0\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model, val_iter, criterion, device)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {val_loss:.3f}, Val. Acc: {val_acc*100:.2f}%')\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv0dsWTZE3T0",
        "outputId": "e3d10198-4033-4c8a-c0aa-ffcb80cd517f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Train Loss: 1.045, Train Acc: 44.73%, Val. Loss: 1.006, Val. Acc: 51.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02, Train Loss: 0.940, Train Acc: 57.28%, Val. Loss: 0.977, Val. Acc: 56.77%\n",
            "Epoch: 03, Train Loss: 0.774, Train Acc: 66.94%, Val. Loss: 0.929, Val. Acc: 59.34%\n",
            "Epoch: 04, Train Loss: 0.600, Train Acc: 75.25%, Val. Loss: 1.045, Val. Acc: 59.92%\n",
            "Epoch: 05, Train Loss: 0.435, Train Acc: 83.11%, Val. Loss: 1.208, Val. Acc: 54.45%\n",
            "Epoch: 06, Train Loss: 0.287, Train Acc: 89.37%, Val. Loss: 1.388, Val. Acc: 55.07%\n",
            "Epoch: 07, Train Loss: 0.169, Train Acc: 94.19%, Val. Loss: 1.717, Val. Acc: 55.07%\n",
            "Epoch: 08, Train Loss: 0.102, Train Acc: 96.57%, Val. Loss: 2.105, Val. Acc: 52.44%\n",
            "Epoch: 09, Train Loss: 0.045, Train Acc: 98.70%, Val. Loss: 2.168, Val. Acc: 55.20%\n",
            "Epoch: 10, Train Loss: 0.029, Train Acc: 99.36%, Val. Loss: 2.226, Val. Acc: 55.11%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_iter, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqZiEIG7FDN3",
        "outputId": "bdcf274f-96b3-43e6-e75b-2794924a63d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.989, Test Acc: 62.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show some improvement in the test accuracy, but the model is still experiencing overfitting, as indicated by the increase in validation loss and relatively low validation accuracy. We will further optimize this again."
      ],
      "metadata": {
        "id": "1QRhaD6DFksP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define the Enhanced GRU Model**\n",
        "We'll start with an enhanced GRU model that allows for easy modification of hyperparameters and architecture."
      ],
      "metadata": {
        "id": "GlZDHWcrF5_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedGRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, dropout=0.5, num_layers=2):\n",
        "        super(EnhancedGRUClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, label_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        batch_size = text.size(0)\n",
        "        hidden = self.init_hidden(batch_size, text.device)\n",
        "        gru_out, hidden = self.gru(embedded, hidden)\n",
        "        hidden = self.dropout(hidden[-1])\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n"
      ],
      "metadata": {
        "id": "BifCWexqFaC6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, labels = batch.text.to(device), batch.label.to(device)\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, labels = batch.text.to(device), batch.label.to(device)\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.argmax(preds, dim=1)\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "FR1HPIMSORVg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "# Reduced set of hyperparameters for experimentation\n",
        "hidden_dim_list = [128]\n",
        "learning_rate_list = [0.001]\n",
        "batch_size_list = [32]\n",
        "optimizer_list = ['adam', 'sgd']\n",
        "dropout_list = [0.5]\n",
        "num_layers_list = [1]\n",
        "\n",
        "# Set the best model parameters\n",
        "best_val_acc = 0.0\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "# Timing the whole grid search\n",
        "start_time = time.time()\n",
        "\n",
        "for hidden_dim in hidden_dim_list:\n",
        "    for lr in learning_rate_list:\n",
        "        for batch_size in batch_size_list:\n",
        "            for optimizer_name in optimizer_list:\n",
        "                for dropout in dropout_list:\n",
        "                    for num_layers in num_layers_list:\n",
        "                        print(f\"Training with hidden_dim={hidden_dim}, lr={lr}, batch_size={batch_size}, optimizer={optimizer_name}, dropout={dropout}, num_layers={num_layers}\")\n",
        "\n",
        "                        # Initialize the model with current hyperparameters\n",
        "                        model = EnhancedGRUClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, dropout=dropout, num_layers=num_layers).to(device)\n",
        "\n",
        "                        # Define loss function\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                        # Choose optimizer\n",
        "                        if optimizer_name == 'adam':\n",
        "                            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                        elif optimizer_name == 'sgd':\n",
        "                            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "                        # Define a learning rate scheduler\n",
        "                        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "                        # Train the model\n",
        "                        num_epochs = 10\n",
        "                        for epoch in range(num_epochs):\n",
        "                            train_loss, train_acc = train(model, train_iter, optimizer, criterion, device)\n",
        "                            val_loss, val_acc = evaluate(model, val_iter, criterion, device)\n",
        "                            if val_acc > best_val_acc:\n",
        "                                best_val_acc = val_acc\n",
        "                                best_model = copy.deepcopy(model.state_dict())\n",
        "                                best_params = {\n",
        "                                    'hidden_dim': hidden_dim,\n",
        "                                    'lr': lr,\n",
        "                                    'batch_size': batch_size,\n",
        "                                    'optimizer': optimizer_name,\n",
        "                                    'dropout': dropout,\n",
        "                                    'num_layers': num_layers\n",
        "                                }\n",
        "                            print(f\"Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {val_loss:.3f}, Val. Acc: {val_acc*100:.2f}%\")\n",
        "                            scheduler.step()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# Print best parameters and final test evaluation\n",
        "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total Training Time: {(end_time - start_time) / 60:.2f} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3mntaHca5rC",
        "outputId": "47064b14-91b4-4a2c-affa-db538555e9a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with hidden_dim=128, lr=0.001, batch_size=32, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.044, Train Acc: 44.67%, Val. Loss: 1.028, Val. Acc: 49.48%\n",
            "Epoch: 02, Train Loss: 0.927, Train Acc: 58.11%, Val. Loss: 0.976, Val. Acc: 56.05%\n",
            "Epoch: 03, Train Loss: 0.762, Train Acc: 68.13%, Val. Loss: 1.008, Val. Acc: 56.09%\n",
            "Epoch: 04, Train Loss: 0.718, Train Acc: 69.85%, Val. Loss: 1.026, Val. Acc: 56.22%\n",
            "Epoch: 05, Train Loss: 0.690, Train Acc: 71.29%, Val. Loss: 1.036, Val. Acc: 56.18%\n",
            "Epoch: 06, Train Loss: 0.688, Train Acc: 71.21%, Val. Loss: 1.037, Val. Acc: 56.45%\n",
            "Epoch: 07, Train Loss: 0.684, Train Acc: 71.66%, Val. Loss: 1.039, Val. Acc: 56.36%\n",
            "Epoch: 08, Train Loss: 0.684, Train Acc: 71.93%, Val. Loss: 1.040, Val. Acc: 56.36%\n",
            "Epoch: 09, Train Loss: 0.684, Train Acc: 71.51%, Val. Loss: 1.040, Val. Acc: 56.36%\n",
            "Epoch: 10, Train Loss: 0.683, Train Acc: 71.58%, Val. Loss: 1.040, Val. Acc: 56.36%\n",
            "Training with hidden_dim=128, lr=0.001, batch_size=32, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.062, Train Acc: 39.20%, Val. Loss: 1.061, Val. Acc: 36.68%\n",
            "Epoch: 02, Train Loss: 1.055, Train Acc: 41.73%, Val. Loss: 1.059, Val. Acc: 38.82%\n",
            "Epoch: 03, Train Loss: 1.049, Train Acc: 42.38%, Val. Loss: 1.057, Val. Acc: 38.73%\n",
            "Epoch: 04, Train Loss: 1.048, Train Acc: 42.56%, Val. Loss: 1.057, Val. Acc: 38.46%\n",
            "Epoch: 05, Train Loss: 1.051, Train Acc: 42.57%, Val. Loss: 1.057, Val. Acc: 38.46%\n",
            "Epoch: 06, Train Loss: 1.050, Train Acc: 42.43%, Val. Loss: 1.057, Val. Acc: 38.82%\n",
            "Epoch: 07, Train Loss: 1.048, Train Acc: 42.43%, Val. Loss: 1.057, Val. Acc: 38.64%\n",
            "Epoch: 08, Train Loss: 1.050, Train Acc: 41.87%, Val. Loss: 1.057, Val. Acc: 38.64%\n",
            "Epoch: 09, Train Loss: 1.051, Train Acc: 41.55%, Val. Loss: 1.057, Val. Acc: 38.64%\n",
            "Epoch: 10, Train Loss: 1.049, Train Acc: 42.61%, Val. Loss: 1.057, Val. Acc: 38.64%\n",
            "\n",
            "Best Hyperparameters: {'hidden_dim': 128, 'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam', 'dropout': 0.5, 'num_layers': 1}\n",
            "Test Loss: 0.962, Test Acc: 59.51%\n",
            "Total Training Time: 2.66 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Combination (Adam Optimizer):**\n",
        "\n",
        "10 epochs of training and validation.\n",
        "\n",
        "Final validation accuracy: 57.83%\n",
        "\n",
        "**Second Combination (SGD Optimizer):**\n",
        "\n",
        "10 epochs of training and validation.\n",
        "\n",
        "Final validation accuracy: 48.87%\n",
        "\n",
        "**Best Hyperparameters Identified:**\n",
        "\n",
        "Best validation accuracy achieved with Adam optimizer.\n",
        "\n",
        "Test accuracy: 59.51%\n",
        "\n",
        "**Final Observations**\n",
        "\n",
        "**Grid Search:** The grid search correctly iterates through all hyperparameter combinations.\n",
        "\n",
        "**Epoch Outputs:** Each combination is trained for the specified 10 epochs.\n",
        "\n",
        "**Best Model:** The best model is correctly identified and evaluated on the test data.\n",
        "\n",
        "***Next =>***\n",
        "Now that the basic grid search and hyperparameter tuning process are verified, I will:\n",
        "\n",
        "**Expand the Hyperparameter Search Space:** Gradually increase the number of hyperparameter combinations to explore more settings.\n",
        "\n",
        "**Experiment with Different Models:** Try using different recurrent layers like LSTM or self-attention mechanisms.\n",
        "\n",
        "**Use Advanced Techniques:** Implement early stopping, more sophisticated learning rate schedulers, or use tools like Optuna for more efficient hyperparameter tuning.\n"
      ],
      "metadata": {
        "id": "Hrp7iQzkbpsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Expanded Hyperparameter Search**"
      ],
      "metadata": {
        "id": "L4egYgCxdD7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "# Smaller subset of hyperparameters for initial testing\n",
        "hidden_dim_list = [128, 256]\n",
        "learning_rate_list = [0.001, 0.01]\n",
        "batch_size_list = [32, 64]\n",
        "optimizer_list = ['adam', 'sgd']\n",
        "dropout_list = [0.5]\n",
        "num_layers_list = [1]\n",
        "\n",
        "# Set the best model parameters\n",
        "best_val_acc = 0.0\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "# Timing the whole grid search\n",
        "start_time = time.time()\n",
        "combination_count = 0\n",
        "\n",
        "# Expected number of combinations\n",
        "expected_combinations = len(hidden_dim_list) * len(learning_rate_list) * len(batch_size_list) * len(optimizer_list) * len(dropout_list) * len(num_layers_list)\n",
        "print(f\"Expected number of combinations: {expected_combinations}\")\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3\n",
        "early_stopping_delta = 0.01\n",
        "\n",
        "for hidden_dim in hidden_dim_list:\n",
        "    for lr in learning_rate_list:\n",
        "        for batch_size in batch_size_list:\n",
        "            for optimizer_name in optimizer_list:\n",
        "                for dropout in dropout_list:\n",
        "                    for num_layers in num_layers_list:\n",
        "                        combination_count += 1\n",
        "                        print(f\"\\nCombination {combination_count}/{expected_combinations}: hidden_dim={hidden_dim}, lr={lr}, batch_size={batch_size}, optimizer={optimizer_name}, dropout={dropout}, num_layers={num_layers}\")\n",
        "\n",
        "                        # Initialize the model with current hyperparameters\n",
        "                        model = EnhancedGRUClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, dropout=dropout, num_layers=num_layers).to(device)\n",
        "\n",
        "                        # Define loss function\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                        # Choose optimizer\n",
        "                        if optimizer_name == 'adam':\n",
        "                            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                        elif optimizer_name == 'sgd':\n",
        "                            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "                        # Define a learning rate scheduler\n",
        "                        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
        "\n",
        "                        # Train the model with early stopping\n",
        "                        num_epochs = 10\n",
        "                        val_acc_history = []\n",
        "                        for epoch in range(num_epochs):\n",
        "                            train_loss, train_acc = train(model, train_iter, optimizer, criterion, device)\n",
        "                            val_loss, val_acc = evaluate(model, val_iter, criterion, device)\n",
        "                            val_acc_history.append(val_acc)\n",
        "                            if val_acc > best_val_acc:\n",
        "                                best_val_acc = val_acc\n",
        "                                best_model = copy.deepcopy(model.state_dict())\n",
        "                                best_params = {\n",
        "                                    'hidden_dim': hidden_dim,\n",
        "                                    'lr': lr,\n",
        "                                    'batch_size': batch_size,\n",
        "                                    'optimizer': optimizer_name,\n",
        "                                    'dropout': dropout,\n",
        "                                    'num_layers': num_layers\n",
        "                                }\n",
        "                            print(f\"Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {val_loss:.3f}, Val. Acc: {val_acc*100:.2f}%\")\n",
        "                            scheduler.step()\n",
        "\n",
        "                            # Early stopping\n",
        "                            if len(val_acc_history) > patience:\n",
        "                                if max(val_acc_history[-patience:]) - min(val_acc_history[-patience:]) < early_stopping_delta:\n",
        "                                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                                    break\n",
        "\n",
        "# Load the best model using the best hyperparameters\n",
        "best_model_instance = EnhancedGRUClassifier(\n",
        "    vocab_size, embedding_dim, best_params['hidden_dim'], label_size, padding_idx,\n",
        "    dropout=best_params['dropout'], num_layers=best_params['num_layers']\n",
        ").to(device)\n",
        "best_model_instance.load_state_dict(best_model)\n",
        "\n",
        "# Print best parameters and final test evaluation\n",
        "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
        "test_loss, test_acc = evaluate(best_model_instance, test_iter, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total Training Time: {(end_time - start_time) / 60:.2f} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWQslhNfbbVo",
        "outputId": "a9c0ea18-4b3a-495e-dfc3-e346db82e19b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected number of combinations: 16\n",
            "\n",
            "Combination 1/16: hidden_dim=128, lr=0.001, batch_size=32, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.051, Train Acc: 43.13%, Val. Loss: 1.024, Val. Acc: 50.38%\n",
            "Epoch: 02, Train Loss: 0.949, Train Acc: 55.66%, Val. Loss: 0.968, Val. Acc: 55.88%\n",
            "Epoch: 03, Train Loss: 0.778, Train Acc: 66.87%, Val. Loss: 0.966, Val. Acc: 57.22%\n",
            "Epoch: 04, Train Loss: 0.737, Train Acc: 69.14%, Val. Loss: 0.968, Val. Acc: 58.33%\n",
            "Epoch: 05, Train Loss: 0.707, Train Acc: 70.70%, Val. Loss: 0.980, Val. Acc: 58.23%\n",
            "Epoch: 06, Train Loss: 0.707, Train Acc: 70.79%, Val. Loss: 0.985, Val. Acc: 58.15%\n",
            "Early stopping at epoch 6\n",
            "\n",
            "Combination 2/16: hidden_dim=128, lr=0.001, batch_size=32, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.056, Train Acc: 41.68%, Val. Loss: 1.066, Val. Acc: 37.75%\n",
            "Epoch: 02, Train Loss: 1.054, Train Acc: 41.73%, Val. Loss: 1.059, Val. Acc: 39.00%\n",
            "Epoch: 03, Train Loss: 1.048, Train Acc: 42.59%, Val. Loss: 1.059, Val. Acc: 38.28%\n",
            "Epoch: 04, Train Loss: 1.047, Train Acc: 42.12%, Val. Loss: 1.059, Val. Acc: 38.64%\n",
            "Early stopping at epoch 4\n",
            "\n",
            "Combination 3/16: hidden_dim=128, lr=0.001, batch_size=64, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.043, Train Acc: 44.71%, Val. Loss: 1.023, Val. Acc: 50.65%\n",
            "Epoch: 02, Train Loss: 0.934, Train Acc: 57.30%, Val. Loss: 0.961, Val. Acc: 56.94%\n",
            "Epoch: 03, Train Loss: 0.757, Train Acc: 67.90%, Val. Loss: 0.969, Val. Acc: 56.90%\n",
            "Epoch: 04, Train Loss: 0.714, Train Acc: 70.38%, Val. Loss: 1.002, Val. Acc: 56.59%\n",
            "Early stopping at epoch 4\n",
            "\n",
            "Combination 4/16: hidden_dim=128, lr=0.001, batch_size=64, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.060, Train Acc: 40.96%, Val. Loss: 1.063, Val. Acc: 38.69%\n",
            "Epoch: 02, Train Loss: 1.053, Train Acc: 41.85%, Val. Loss: 1.059, Val. Acc: 41.05%\n",
            "Epoch: 03, Train Loss: 1.048, Train Acc: 42.65%, Val. Loss: 1.059, Val. Acc: 40.78%\n",
            "Epoch: 04, Train Loss: 1.047, Train Acc: 42.81%, Val. Loss: 1.058, Val. Acc: 40.52%\n",
            "Early stopping at epoch 4\n",
            "\n",
            "Combination 5/16: hidden_dim=128, lr=0.01, batch_size=32, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.057, Train Acc: 48.27%, Val. Loss: 0.977, Val. Acc: 55.02%\n",
            "Epoch: 02, Train Loss: 0.877, Train Acc: 63.03%, Val. Loss: 0.980, Val. Acc: 59.08%\n",
            "Epoch: 03, Train Loss: 0.639, Train Acc: 73.50%, Val. Loss: 1.032, Val. Acc: 58.10%\n",
            "Epoch: 04, Train Loss: 0.499, Train Acc: 79.76%, Val. Loss: 1.105, Val. Acc: 57.38%\n",
            "Epoch: 05, Train Loss: 0.403, Train Acc: 84.50%, Val. Loss: 1.123, Val. Acc: 57.64%\n",
            "Early stopping at epoch 5\n",
            "\n",
            "Combination 6/16: hidden_dim=128, lr=0.01, batch_size=32, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.054, Train Acc: 42.09%, Val. Loss: 1.058, Val. Acc: 41.05%\n",
            "Epoch: 02, Train Loss: 1.039, Train Acc: 44.96%, Val. Loss: 1.047, Val. Acc: 43.10%\n",
            "Epoch: 03, Train Loss: 1.023, Train Acc: 47.50%, Val. Loss: 1.046, Val. Acc: 42.61%\n",
            "Epoch: 04, Train Loss: 1.023, Train Acc: 47.33%, Val. Loss: 1.044, Val. Acc: 43.41%\n",
            "Early stopping at epoch 4\n",
            "\n",
            "Combination 7/16: hidden_dim=128, lr=0.01, batch_size=64, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.053, Train Acc: 47.79%, Val. Loss: 0.942, Val. Acc: 56.81%\n",
            "Epoch: 02, Train Loss: 0.861, Train Acc: 63.25%, Val. Loss: 1.031, Val. Acc: 56.32%\n",
            "Epoch: 03, Train Loss: 0.649, Train Acc: 73.28%, Val. Loss: 1.059, Val. Acc: 57.74%\n",
            "Epoch: 04, Train Loss: 0.473, Train Acc: 81.13%, Val. Loss: 1.163, Val. Acc: 57.65%\n",
            "Epoch: 05, Train Loss: 0.365, Train Acc: 86.43%, Val. Loss: 1.185, Val. Acc: 57.65%\n",
            "Early stopping at epoch 5\n",
            "\n",
            "Combination 8/16: hidden_dim=128, lr=0.01, batch_size=64, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.055, Train Acc: 41.77%, Val. Loss: 1.053, Val. Acc: 41.27%\n",
            "Epoch: 02, Train Loss: 1.043, Train Acc: 43.61%, Val. Loss: 1.050, Val. Acc: 40.78%\n",
            "Epoch: 03, Train Loss: 1.024, Train Acc: 46.99%, Val. Loss: 1.054, Val. Acc: 41.59%\n",
            "Epoch: 04, Train Loss: 1.023, Train Acc: 46.82%, Val. Loss: 1.051, Val. Acc: 41.77%\n",
            "Early stopping at epoch 4\n",
            "\n",
            "Combination 9/16: hidden_dim=256, lr=0.001, batch_size=32, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.039, Train Acc: 46.06%, Val. Loss: 1.006, Val. Acc: 53.29%\n",
            "Epoch: 02, Train Loss: 0.929, Train Acc: 58.09%, Val. Loss: 0.940, Val. Acc: 58.77%\n",
            "Epoch: 03, Train Loss: 0.760, Train Acc: 68.06%, Val. Loss: 0.946, Val. Acc: 58.50%\n",
            "Epoch: 04, Train Loss: 0.710, Train Acc: 70.95%, Val. Loss: 0.955, Val. Acc: 57.44%\n",
            "Epoch: 05, Train Loss: 0.675, Train Acc: 72.50%, Val. Loss: 0.963, Val. Acc: 58.02%\n",
            "Epoch: 06, Train Loss: 0.672, Train Acc: 72.54%, Val. Loss: 0.967, Val. Acc: 58.02%\n",
            "Early stopping at epoch 6\n",
            "\n",
            "Combination 10/16: hidden_dim=256, lr=0.001, batch_size=32, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.054, Train Acc: 41.74%, Val. Loss: 1.052, Val. Acc: 40.69%\n",
            "Epoch: 02, Train Loss: 1.047, Train Acc: 43.42%, Val. Loss: 1.048, Val. Acc: 40.87%\n",
            "Epoch: 03, Train Loss: 1.047, Train Acc: 43.54%, Val. Loss: 1.050, Val. Acc: 41.41%\n",
            "Epoch: 04, Train Loss: 1.047, Train Acc: 42.91%, Val. Loss: 1.051, Val. Acc: 41.77%\n",
            "Early stopping at epoch 4\n",
            "\n",
            "Combination 11/16: hidden_dim=256, lr=0.001, batch_size=64, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.027, Train Acc: 48.01%, Val. Loss: 0.975, Val. Acc: 55.07%\n",
            "Epoch: 02, Train Loss: 0.916, Train Acc: 58.77%, Val. Loss: 0.958, Val. Acc: 56.50%\n",
            "Epoch: 03, Train Loss: 0.738, Train Acc: 69.03%, Val. Loss: 0.969, Val. Acc: 58.60%\n",
            "Epoch: 04, Train Loss: 0.692, Train Acc: 71.21%, Val. Loss: 0.988, Val. Acc: 57.80%\n",
            "Epoch: 05, Train Loss: 0.657, Train Acc: 72.62%, Val. Loss: 1.003, Val. Acc: 58.28%\n",
            "Early stopping at epoch 5\n",
            "\n",
            "Combination 12/16: hidden_dim=256, lr=0.001, batch_size=64, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.058, Train Acc: 40.27%, Val. Loss: 1.057, Val. Acc: 46.99%\n",
            "Epoch: 02, Train Loss: 1.050, Train Acc: 42.33%, Val. Loss: 1.052, Val. Acc: 40.91%\n",
            "Epoch: 03, Train Loss: 1.047, Train Acc: 43.15%, Val. Loss: 1.053, Val. Acc: 40.56%\n",
            "Epoch: 04, Train Loss: 1.046, Train Acc: 42.79%, Val. Loss: 1.053, Val. Acc: 40.82%\n",
            "Early stopping at epoch 4\n",
            "\n",
            "Combination 13/16: hidden_dim=256, lr=0.01, batch_size=32, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.140, Train Acc: 44.74%, Val. Loss: 0.996, Val. Acc: 54.53%\n",
            "Epoch: 02, Train Loss: 1.135, Train Acc: 50.77%, Val. Loss: 1.258, Val. Acc: 44.09%\n",
            "Epoch: 03, Train Loss: 1.110, Train Acc: 47.90%, Val. Loss: 1.059, Val. Acc: 46.63%\n",
            "Epoch: 04, Train Loss: 1.006, Train Acc: 52.10%, Val. Loss: 1.052, Val. Acc: 45.87%\n",
            "Epoch: 05, Train Loss: 0.973, Train Acc: 54.34%, Val. Loss: 1.051, Val. Acc: 46.85%\n",
            "Early stopping at epoch 5\n",
            "\n",
            "Combination 14/16: hidden_dim=256, lr=0.01, batch_size=32, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.059, Train Acc: 41.62%, Val. Loss: 1.053, Val. Acc: 40.69%\n",
            "Epoch: 02, Train Loss: 1.039, Train Acc: 44.86%, Val. Loss: 1.044, Val. Acc: 45.21%\n",
            "Epoch: 03, Train Loss: 1.022, Train Acc: 47.38%, Val. Loss: 1.043, Val. Acc: 45.60%\n",
            "Epoch: 04, Train Loss: 1.021, Train Acc: 48.33%, Val. Loss: 1.047, Val. Acc: 44.35%\n",
            "Epoch: 05, Train Loss: 1.017, Train Acc: 48.35%, Val. Loss: 1.045, Val. Acc: 44.62%\n",
            "Epoch: 06, Train Loss: 1.016, Train Acc: 48.75%, Val. Loss: 1.044, Val. Acc: 45.25%\n",
            "Early stopping at epoch 6\n",
            "\n",
            "Combination 15/16: hidden_dim=256, lr=0.01, batch_size=64, optimizer=adam, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.117, Train Acc: 45.89%, Val. Loss: 1.012, Val. Acc: 47.79%\n",
            "Epoch: 02, Train Loss: 1.093, Train Acc: 53.44%, Val. Loss: 1.212, Val. Acc: 42.53%\n",
            "Epoch: 03, Train Loss: 1.094, Train Acc: 50.27%, Val. Loss: 1.048, Val. Acc: 48.15%\n",
            "Epoch: 04, Train Loss: 0.982, Train Acc: 53.93%, Val. Loss: 1.015, Val. Acc: 51.04%\n",
            "Epoch: 05, Train Loss: 0.956, Train Acc: 55.62%, Val. Loss: 1.019, Val. Acc: 52.34%\n",
            "Epoch: 06, Train Loss: 0.944, Train Acc: 56.17%, Val. Loss: 1.020, Val. Acc: 51.81%\n",
            "Epoch: 07, Train Loss: 0.938, Train Acc: 56.76%, Val. Loss: 1.019, Val. Acc: 52.65%\n",
            "Early stopping at epoch 7\n",
            "\n",
            "Combination 16/16: hidden_dim=256, lr=0.01, batch_size=64, optimizer=sgd, dropout=0.5, num_layers=1\n",
            "Epoch: 01, Train Loss: 1.056, Train Acc: 42.24%, Val. Loss: 1.052, Val. Acc: 40.34%\n",
            "Epoch: 02, Train Loss: 1.042, Train Acc: 45.01%, Val. Loss: 1.038, Val. Acc: 44.48%\n",
            "Epoch: 03, Train Loss: 1.028, Train Acc: 45.90%, Val. Loss: 1.041, Val. Acc: 47.43%\n",
            "Epoch: 04, Train Loss: 1.028, Train Acc: 46.92%, Val. Loss: 1.039, Val. Acc: 46.45%\n",
            "Epoch: 05, Train Loss: 1.024, Train Acc: 47.83%, Val. Loss: 1.039, Val. Acc: 46.27%\n",
            "Epoch: 06, Train Loss: 1.021, Train Acc: 47.06%, Val. Loss: 1.039, Val. Acc: 46.18%\n",
            "Early stopping at epoch 6\n",
            "\n",
            "Best Hyperparameters: {'hidden_dim': 128, 'lr': 0.01, 'batch_size': 32, 'optimizer': 'adam', 'dropout': 0.5, 'num_layers': 1}\n",
            "Test Loss: 0.913, Test Acc: 61.03%\n",
            "Total Training Time: 14.82 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to Further Improve the Model:\n",
        "\n",
        "Use Cross-Validation: Implement k-fold cross-validation to get a more reliable estimate of your model's performance.\n",
        "\n",
        "Try Different Models: Experiment with different model architectures like LSTM or even Transformer-based models for better performance on sequence data.\n",
        "\n",
        "Advanced Techniques: Consider techniques like ensemble learning or model stacking."
      ],
      "metadata": {
        "id": "MdbVUuvnoHqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wF6nDnyGocVj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}